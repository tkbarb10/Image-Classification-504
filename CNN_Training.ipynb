{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training: Real vs AI-Generated Image Detection\n",
    "\n",
    "EfficientNetB0 transfer learning and hyperparameter tuning for binary image classification.\n",
    "\n",
    "This notebook is designed to run on a GPU environment. It loads data from Hugging Face\n",
    "and saves the trained model back to Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import warnings\n",
    "\n",
    "from huggingface_hub import ModelCard, ModelCardData, HfApi\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.layers import (\n",
    "    GlobalAveragePooling2D, GlobalMaxPooling2D,\n",
    "    Dropout, Dense, BatchNormalization,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from config import EXPERIMENTS_DIR, MODELS_DIR, LOG_DIR, HF_MODEL_REPO\n",
    "from utils import load_arrays\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_arrays(\"train\")\n",
    "X_val, y_val = load_arrays(\"validation\")\n",
    "X_test, y_test = load_arrays(\"test\")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tb_log_dir = str(LOG_DIR / \"cnn\" / run_id)\n",
    "\n",
    "tensorboard_cb = TensorBoard(\n",
    "    log_dir=tb_log_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    "    update_freq=\"epoch\",\n",
    "    profile_batch=0,\n",
    ")\n",
    "\n",
    "print(f\"TensorBoard logs: {tb_log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Train Top Classifier\n",
    "\n",
    "Freeze the EfficientNetB0 base and train only the classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model with ImageNet weights\n",
    "base_model = EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# Stage 1: Train top only\n",
    "base_model.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.inputs, outputs=output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Stage 1: Training top classifier only...\")\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Fine-Tune Deeper Layers\n",
    "\n",
    "Unfreeze the last 100 layers of EfficientNetB0 and fine-tune with a lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-100]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    if isinstance(layer, BatchNormalization):\n",
    "        layer.trainable = False\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=5, verbose=1, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Stage 2: Fine-tuning deeper layers...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, tensorboard_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history)[['accuracy', 'loss', 'val_accuracy', 'val_loss']].plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.title(\"Stage 2 Fine-Tuning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(X_val)\n",
    "y_pred = (y_pred_proba > 0.5).astype(\"int32\")\n",
    "print(\"Validation Results:\")\n",
    "print(classification_report(y_val, y_pred, target_names=[\"AI\", \"Human\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(\"int32\")\n",
    "print(\"Test Results:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"AI\", \"Human\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetHyperModel(kt.HyperModel):\n",
    "  def __init__(self, input_shape, num_classes=1):\n",
    "    self.input_shape = input_shape\n",
    "    self.num_classes = num_classes\n",
    "\n",
    "  def build(self, hp):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False)\n",
    "    base_model.trainable = False\n",
    "\n",
    "    input = tf.keras.Input(shape=self.input_shape, name=\"our_input_layer\")\n",
    "    augmentation_layer = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\n",
    "            mode=hp.Choice('flip_mode', ['horizontal', 'vertical', 'horizontal_and_vertical'])\n",
    "            ),\n",
    "        tf.keras.layers.RandomRotation(\n",
    "            factor=hp.Float(\"rotation_factor\", min_value=0, max_value=.3, step=.1),\n",
    "            fill_mode=hp.Choice(\"fill_mode\", ['constant', 'reflect', 'wrap', 'nearest']),\n",
    "            interpolation=hp.Choice(\"interpolation\", ['nearest', 'bilinear'])\n",
    "            ),\n",
    "        tf.keras.layers.RandomContrast(\n",
    "            factor=hp.Float(\"contrast_factor\", min_value=0, max_value=1, step=.2)\n",
    "        )\n",
    "    ], name='augmentation_layer')\n",
    "\n",
    "    x = augmentation_layer(input)\n",
    "    x = base_model(x, training=False)\n",
    "\n",
    "    pooling = hp.Choice(\"pooling\", [\"avg\", \"max\"])\n",
    "    if pooling == \"avg\":\n",
    "      x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    else:\n",
    "      x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "\n",
    "    if hp.Boolean(\"add_dense\"):\n",
    "      x = tf.keras.layers.Dense(\n",
    "          units=hp.Int(\"dense_units\", min_value=64, max_value=256, step=64),\n",
    "          activation='relu', name = \"our_dense_layer\"\n",
    "      )(x)\n",
    "\n",
    "    x = tf.keras.layers.Dropout(rate=hp.Float('head_dropout', min_value=0, max_value=.5, step=.1), name=\"Dropout\")(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(units=self.num_classes, activation='sigmoid', name=\"output_layer\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "    lr = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "    optimizer_choice = hp.Choice(\"optimizer\", [\"adam\", \"rmsprop\", \"adamw\"])\n",
    "\n",
    "    if optimizer_choice == \"adam\":\n",
    "      from tensorflow.keras.optimizers import Adam\n",
    "      optimizer = Adam(learning_rate=lr)\n",
    "    elif optimizer_choice == \"rmsprop\":\n",
    "      from tensorflow.keras.optimizers import RMSprop\n",
    "      optimizer = RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "      from tensorflow.keras.optimizers import AdamW\n",
    "      optimizer = AdamW(learning_rate=lr, weight_decay=hp.Float('adamw_weight_decay', min_value=1e-7, max_value=.01, sampling='log'))\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5, name='accuracy'),\n",
    "                           tf.keras.metrics.AUC(name='AUC')]\n",
    "                  )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    hypermodel=EfficientNetHyperModel(input_shape=(224, 224, 3)),\n",
    "    objective='val_accuracy',\n",
    "    seed=10,\n",
    "    max_epochs=20,\n",
    "    hyperband_iterations=2,\n",
    "    executions_per_trial=2,\n",
    "    directory=str(EXPERIMENTS_DIR),\n",
    "    project_name='full_hyperparm_tuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Hyperparameter Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_tb = TensorBoard(\n",
    "    log_dir=str(LOG_DIR / \"tuner\"),\n",
    "    histogram_freq=0,\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "tuner.search(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping, tuner_tb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "print(\"Best hyperparameters:\")\n",
    "print(best_hp.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "my_model = EfficientNetHyperModel(input_shape=(224, 224, 3))\n",
    "best_model = my_model.build(best_hp)\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally load a previously saved model instead\n",
    "# best_model = tf.keras.models.load_model(MODELS_DIR / \"efficient_net_top_model.keras\")\n",
    "# best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune Best Model\n",
    "\n",
    "Unfreeze the last 50 layers and fine-tune with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = best_model.get_layer('efficientnetb0')\n",
    "\n",
    "base_model.trainable = True\n",
    "\n",
    "for layer in base_model.layers[:-50]:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_tb = TensorBoard(\n",
    "    log_dir=str(LOG_DIR / \"cnn\" / f\"finetune-{run_id}\"),\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    "    update_freq=\"epoch\",\n",
    "    profile_batch=0,\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "best_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping, finetune_tb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history)[['accuracy', 'loss', 'val_accuracy', 'val_loss']].plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.title(\"Best Model Fine-Tuning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = best_model.predict(X_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(\"int32\")\n",
    "print(\"Test Results:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"AI\", \"Human\"]))\n",
    "\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"Test AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model using Keras native HF integration\n",
    "best_model.save(f\"hf://{HF_MODEL_REPO}\")\n",
    "print(f\"Model saved to hf://{HF_MODEL_REPO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push a detailed model card\n",
    "card_data = ModelCardData(\n",
    "    library_name=\"keras\",\n",
    "    license=\"mit\",\n",
    "    tags=[\"image-classification\", \"ai-generated-image-detection\", \"efficientnet\", \"transfer-learning\"],\n",
    "    datasets=[\"tkbarb10/ADS504-Image-Arrays\"],\n",
    "    model_name=\"ADS504 EfficientNetB0 - AI vs Human Image Classifier\",\n",
    ")\n",
    "\n",
    "card_content = f\"\"\"---\n",
    "{card_data.to_yaml()}\n",
    "---\n",
    "\n",
    "# ADS504 EfficientNetB0: Real vs AI-Generated Image Classifier\n",
    "\n",
    "Fine-tuned EfficientNetB0 for binary classification of AI-generated vs human-created images.\n",
    "\n",
    "## Model Description\n",
    "\n",
    "- **Architecture:** EfficientNetB0 (transfer learning from ImageNet)\n",
    "- **Task:** Binary image classification (AI-generated vs Human-created)\n",
    "- **Input:** 224x224x3 RGB images (uint8)\n",
    "- **Output:** Sigmoid probability (>0.5 = human)\n",
    "\n",
    "## Training Procedure\n",
    "\n",
    "### Stage 1: Train Top Classifier\n",
    "- Frozen EfficientNetB0 base\n",
    "- Added: GlobalAveragePooling2D -> Dropout(0.3) -> Dense(1, sigmoid)\n",
    "- Optimizer: Adam(lr=1e-3)\n",
    "- Epochs: 5\n",
    "\n",
    "### Stage 2: Fine-Tune\n",
    "- Unfroze last 50 layers of EfficientNetB0\n",
    "- Kept BatchNormalization layers frozen\n",
    "- Optimizer: Adam(lr=1e-5)\n",
    "- EarlyStopping: patience=5, restore_best_weights=True\n",
    "- Max epochs: 50\n",
    "\n",
    "### Hyperparameter Search (Keras Tuner Hyperband)\n",
    "Parameters tuned: flip mode, rotation factor, contrast factor, pooling type,\n",
    "dense layer presence/units, dropout rate, optimizer (Adam/RMSprop/AdamW),\n",
    "learning rate (1e-5 to 1e-2 log scale).\n",
    "\n",
    "## Dataset\n",
    "\n",
    "[tkbarb10/ADS504-Image-Arrays](https://huggingface.co/datasets/tkbarb10/ADS504-Image-Arrays)\n",
    "- Train: 11,998 images | Validation: 1,499 images | Test: 1,500 images\n",
    "- Sources: LAION, Open Images, AI vs Human study\n",
    "\n",
    "## Best Hyperparameters\n",
    "\n",
    "```json\n",
    "{json.dumps(best_hp.values, indent=2)}\n",
    "```\n",
    "\n",
    "## Metrics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Test AUC | {test_auc:.4f} |\n",
    "\n",
    "## Limitations\n",
    "\n",
    "The CNN underperformed classical models (Random Forest: 92% accuracy, AUC 0.98) trained on\n",
    "hand-crafted image features (color statistics, edge density, texture descriptors).\n",
    "\n",
    "## Authors\n",
    "\n",
    "Taylor Kirk, Tommy Baron, Paola Rodriguez - University of San Diego, ADS 504\n",
    "\"\"\"\n",
    "\n",
    "card = ModelCard(card_content)\n",
    "card.push_to_hub(HF_MODEL_REPO)\n",
    "print(\"Model card pushed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload best hyperparameters as a JSON artifact\n",
    "hp_path = MODELS_DIR / \"best_hyperparameters.json\"\n",
    "hp_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(hp_path, \"w\") as f:\n",
    "    json.dump(best_hp.values, f, indent=2)\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=str(hp_path),\n",
    "    path_in_repo=\"best_hyperparameters.json\",\n",
    "    repo_id=HF_MODEL_REPO,\n",
    ")\n",
    "print(\"Hyperparameters uploaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch TensorBoard\n",
    "\n",
    "From the terminal:\n",
    "```bash\n",
    "tensorboard --logdir=logs/\n",
    "```\n",
    "\n",
    "Or inline in Jupyter/Colab:\n",
    "```python\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
